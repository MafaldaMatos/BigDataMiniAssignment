# BigDataMiniAssignment
The goal of this assignment is to compare the time efficiency VS. the size of data, when doing the same task using different code implementations - sequencial, using Pyspark and using Apache Beam. The task given is to import text data from a CSV file, tokenize the words and count them.

The implementations are, in order, bigdata_seq.py, bigdata_pyspark.py and bigdata_beam.py. The test.csv file is an example of one of the datasets we used for the project, and the .pdf file is the mini-report. For completion sake, we also included the code to produce the plots used in the mini-report.
